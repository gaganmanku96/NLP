{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model in KERAS - SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are we using a package?\n",
    "#### The difficulty I felt when I first used was to load the saved model. Attention isn't a layer, it is mathematical operation. When we try to load the model again, it will through an error saying there is no Attention layer. When loading the saving model we'll use get_config method present in the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_self_attention\n",
    "!pip install nlppreprocess\n",
    "## Uncomment this to use glove embedding.\n",
    "# !wget http://nlp.stanford.edu/data/glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# I don't have NVIDIA GPU, this is why I used plaidml to train models on my AMD GPU.\n",
    "os.environ['KERAS_BACKEND'] = \"plaidml.keras.backend\"\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "from keras_self_attention import SeqWeightedAttention\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BidLstm(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    x = SeqWeightedAttention()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(11, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "def make_df(train_path, max_features, maxlen, list_classes):\n",
    "    \n",
    "    # Load dataset\n",
    "    train = pd.read_csv(train_path, delimiter='\\t')\n",
    "    \n",
    "    # Load all reviews into a list\n",
    "    sentences = train[list_classes[0]].values\n",
    "    \n",
    "    # Create tokenizer to extract words from reviews with higher frequency\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    \n",
    "    # Apply the tokenizer to extract top words\n",
    "    tokenizer.fit_on_texts(list(sentences))\n",
    "    \n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    # Pad sentences with 0 either in beginning or end of sentence to make all sentences of equal length.\n",
    "    X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    \n",
    "    # Load target class into list\n",
    "    y = train[list_classes[1]].values\n",
    "    \n",
    "    # ANN's accept target class in form of sparse matrix\n",
    "    # to_categorical convert it into spare matrix form\n",
    "    y = to_categorical(y)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    return X_t, y, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from smart_open import open\n",
    "\n",
    "def make_glovevec(glovepath, max_features, embed_size, word_index, veclen=300):\n",
    "    embeddings_index = {}\n",
    "    f = open(glovepath, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ' '.join(values[:-300])\n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs.reshape(-1)\n",
    "    f.close()\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
    "# !unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 100, 300)          9000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 100, 512)          1140736   \n",
      "_________________________________________________________________\n",
      "seq_weighted_attention_6 (Se (None, 512)               513       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 11)                2827      \n",
      "=================================================================\n",
      "Total params: 10,275,404\n",
      "Trainable params: 1,275,404\n",
      "Non-trainable params: 9,000,000\n",
      "_________________________________________________________________\n",
      "Starting Model Training...\n",
      "Train on 53228 samples, validate on 538 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 1833 of 21341 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 4956 of 21341 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 7174 of 21341 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 10455 of 21341 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 14176 of 21341 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 17615 of 21341 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 20564 of 21341 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   44/53228 [..............................] - ETA: 32:27:27 - loss: 2.2556 - acc: 0.2273"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    max_features = 30000\n",
    "    maxlen = 100\n",
    "    embed_size = 300\n",
    "    list_classes = ['review', 'rating']\n",
    "    \n",
    "    \n",
    "#     print(\"Preparing Data...\")\n",
    "#     xtr, y, word_index = make_df(\"data/train.tsv\",\n",
    "#                                       max_features, maxlen, list_classes)\n",
    "    \n",
    "#     print(\"Creating Glove Embedding...\")\n",
    "#     embedding_vector = make_glovevec(\"glove.42B.300d/glove.42B.300d.txt\",\n",
    "#                                      max_features, embed_size, word_index)\n",
    "    \n",
    "    print(\"Creating Model...\")\n",
    "    model = BidLstm(maxlen, max_features, embed_size, embedding_vector)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    file_path = \".model.hdf5\"\n",
    "    ckpt = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,\n",
    "                           save_best_only=True, mode='min')\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "    \n",
    "    print(\"Starting Model Training...\")\n",
    "    model.fit(xtr, y, batch_size=16, epochs=2, validation_split=0.01, callbacks=[ckpt, early])\n",
    "    \n",
    "    model.save('saved_model.model')\n",
    "    \n",
    "    import pickle\n",
    "    with open('tokenizer.pkl','wb') as file:\n",
    "        pickle.dump(tokenizer, file)\n",
    "    print(\"tokenizer saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
